{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "\n",
    "import pickle as pkl\n",
    "import plotly.express as px # type: ignore\n",
    "import plotly.graph_objects as go # type: ignore\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from StreamlitApp.functions.carga_dataframes import *\n",
    "from ML.escalado_datos import *\n",
    "from ML.gru_rnn import *\n",
    "from StreamlitApp.passwords import pw\n",
    "\n",
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import GRU, Dense, Input, Dropout # type: ignore\n",
    "from tensorflow.keras.callbacks import EarlyStopping # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN GRU\n",
    "\n",
    "Para este modelo importamos el scaler guardado en escalado_datos.py.\n",
    "\n",
    "Luego definimos varias funciones para que:\n",
    "\n",
    "- se creen secuencias en los datos en base a la ventana temporal deseada\n",
    "- se redimensionen esas secuencias adecuadamente\n",
    "- se cree la RNN GRU adaptada a las secuencias. Hemos añadido algunas capas Dropout para evitar el over-fitting.\n",
    "- se predigan los valores de demanda con 1-step y multi-step\n",
    "- se muestren métricas y gráficos relativos al desempeño del modelo en cuestión (luego se implementarán en Streamlit)\n",
    "\n",
    "Entrenamos un modelo con lookback de 30 días y 1000 epochs y lo guardamos en .pkl para usar desde Streamlit. También guardamos un dataframe con métricas y los gráficos de loss-mae para mostrar posteriormente.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el scaler\n",
    "with open(\"../data/data_scaled/scalers/scaler_consumo_anio_DF_DEMANDA.pkl\", \"br\") as file:\n",
    "    scaler = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear secuencias\n",
    "def crea_secuencias(dataframe, target_col, len_secuencia) -> tuple:\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataframe) - len_secuencia):\n",
    "        X.append(dataframe.iloc[i:i+len_secuencia].drop(columns=[target_col]).values) \n",
    "        y.append(dataframe.iloc[i+len_secuencia][target_col]) \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para redimensionar las secuencias\n",
    "def redimensiona(xtrain, xtest, ventana, n_features) -> str:\n",
    "    xtrain = xtrain.reshape((xtrain.shape[0], ventana, n_features))  \n",
    "    xtest = xtest.reshape((xtest.shape[0], ventana, n_features)) \n",
    "    return f\"X_train shape: {xtrain.shape}, X_test shape: {xtest.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construcción del modelo GRU con más capas. Este modelo me parece más realista\n",
    "def crea_gru(input_shape, len_secuencia, xtrain, xtest, ytrain, ytest) -> tuple: #--  agregar metricas en un df\n",
    "    model = Sequential([\n",
    "        Input(shape=(len_secuencia, input_shape)),\n",
    "        GRU(64, activation='tanh'),  \n",
    "        Dropout(0.3), \n",
    "        Dense(1, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='linear') \n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    #early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    #history = model.fit(x=xtrain, y=ytrain, validation_data=(xtest, ytest), epochs=100, verbose=0, callbacks=[early_stopping])  2500 epochs sin early probar\n",
    "    history = model.fit(x=xtrain, y=ytrain, validation_data=(xtest, ytest), epochs=500, verbose=0)\n",
    "    \n",
    "    with open(f\"MODELS/GRU/gru_model{len_secuencia}.pkl\", \"bw\") as file:\n",
    "        pickle.dump(model,file)\n",
    "        return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para graficar loss y mae\n",
    "def grafica_loss_mae(historial) -> None:\n",
    "    fig = px.line(data_frame=historial.history, \n",
    "                  y=['loss', 'val_loss'], \n",
    "                  title='Función de pérdida (loss) basada en el MSE',\n",
    "                  labels={'index': 'Época', 'value': 'Pérdida'})\n",
    "    \n",
    "    fig.update_layout(title_x=0.5, \n",
    "                      template=\"plotly_white\",\n",
    "                      legend_title_text=\"Variables\")\n",
    "    \n",
    "    fig.for_each_trace(lambda x: x.update(name=\"Pérdida Entrenamiento\" if x.name == \"loss\" else \"Pérdida Validación\"))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para predecir 1-step\n",
    "def predice_1step(model, data, scaler, len_secuencia, num_dias) -> tuple:\n",
    "    validation_predictions = []\n",
    "    \n",
    "    i = -num_dias\n",
    "    while len(validation_predictions) < num_dias:\n",
    "        p = model.predict(data[i].reshape(1, len_secuencia, data.shape[2]))[0, 0]\n",
    "        validation_predictions.append(p)\n",
    "        i += 1\n",
    "\n",
    "    # Desescalado\n",
    "    dummy_features = np.zeros((len(validation_predictions), 1))\n",
    "    predictions_with_dummy = np.hstack([np.array(validation_predictions).reshape(-1, 1), dummy_features])\n",
    "    predictions_desescalado = scaler.inverse_transform(predictions_with_dummy)[:, 0]\n",
    "    \n",
    "    return predictions_desescalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para predecir multi-step\n",
    "def predice_multistep(model, data, scaler, len_secuencia, num_dias) -> np.array:\n",
    "    predictions = []\n",
    "    input_seq = data[-1].reshape(1, len_secuencia, data.shape[2])\n",
    "    \n",
    "    for _ in range(num_dias):\n",
    "        pred = model.predict(input_seq)[0, 0]\n",
    "        predictions.append(pred)\n",
    "        input_seq = np.roll(input_seq, shift=-1, axis=1)  \n",
    "        input_seq[0, -1, -1] = pred \n",
    "    \n",
    "    # Desescalado\n",
    "    dummy_features = np.zeros((len(predictions), 1))\n",
    "    predictions_with_dummy = np.hstack([np.array(predictions).reshape(-1, 1), dummy_features])\n",
    "    predictions_desescalado = scaler.inverse_transform(predictions_with_dummy)[:, 0]\n",
    "\n",
    "    return predictions_desescalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar las predicciones 1-step y multi-step\n",
    "def grafica_predicciones(real, pred_1step, pred_multistep) -> None:\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Valores reales\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=real,\n",
    "        mode='lines+markers',\n",
    "        name='Valores Reales',\n",
    "        line=dict(color='blue', width=2),\n",
    "        marker=dict(size=6)\n",
    "    ))\n",
    "\n",
    "    # Predicciones 1-step\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=pred_1step,\n",
    "        mode='lines+markers',\n",
    "        name=f'Predicciones 1-step',\n",
    "        line=dict(color='red', width=2, dash='dot'),\n",
    "        marker=dict(size=6)\n",
    "    ))\n",
    "\n",
    "    # Predicciones multi-step\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=pred_multistep,\n",
    "        mode='lines+markers',\n",
    "        name=f'Predicciones Multi-step',\n",
    "        line=dict(color='green', width=2, dash='dot'),\n",
    "        marker=dict(size=6)\n",
    "    ))\n",
    "\n",
    "    # Configuración de la gráfica\n",
    "    fig.update_layout(\n",
    "        title=\"Predicciones vs Valores Reales\",\n",
    "        title_x=0.5,\n",
    "        xaxis_title=\"Días\",\n",
    "        yaxis_title=\"Demanda (GWh)\",\n",
    "        template=\"plotly_white\",\n",
    "        hovermode=\"x\",\n",
    "        xaxis=dict(\n",
    "            tickvals=list(range(len(pred_1step) + 1)),  \n",
    "            ticktext=[str(i) for i in range(1, len(pred_1step) + 1)]  \n",
    "        ))\n",
    "\n",
    "    fig.show()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar las predicciones futuras\n",
    "def grafica_predicciones_futuras(pred_futuro) -> None:\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Predicciones futuras\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=pred_futuro,\n",
    "        mode='lines+markers',\n",
    "        name=f'Predicciones Futuras',\n",
    "        line=dict(color='orange', width=2, dash='dot'),\n",
    "        marker=dict(size=6)\n",
    "    ))\n",
    "\n",
    "    # Configuración de la gráfica\n",
    "    fig.update_layout(\n",
    "        title=\"Predicciones para Días Futuros\",\n",
    "        title_x=0.5,\n",
    "        xaxis_title=\"Días Futuros\",\n",
    "        yaxis_title=\"Demanda (GWh)\",\n",
    "        template=\"plotly_white\",\n",
    "        hovermode=\"x\",\n",
    "        xaxis=dict(\n",
    "            tickvals=list(range(len(pred_futuro))),\n",
    "            ticktext=[str(i + 1) for i in range(len(pred_futuro))]\n",
    "        ))\n",
    "\n",
    "    fig.show()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Código usado para entrenar los modelos en local y guardalos para su posterior uso en Streamlit con archivo gru_rnn.py\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demanda = carga_dataframes(pw[\"host\"], pw[\"user\"], pw[\"password\"], pw[\"database\"])[1]\n",
    "\n",
    "# df_filtrado simula la función vis_demanda que se usa en main.py, que filtra los datos y luego se usan para las predicciones\n",
    "df_filtrado = df_demanda.copy()\n",
    "df_filtrado['valor_(GWh)'] = df_filtrado['valor_(MWh)'].apply(lambda x : x * 0.001)\n",
    "df_filtrado = df_filtrado[df_filtrado[\"titulo\"] == \"Demanda\"]\n",
    "\n",
    "df = procesar_datos(df_demanda)\n",
    "df = df.drop(columns=\"fecha\")\n",
    "TARGET = df[\"valor_(GWh)\"]\n",
    "\n",
    "# Se usaron ventanas de 7, 15 y 30 días\n",
    "ventana = 7  \n",
    "\n",
    "n_features = len([col for col in df.columns if col != TARGET.name])\n",
    "X, y = crea_secuencias(df, TARGET.name, ventana)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)  # No barajar los datos\n",
    "redimensiona(X_train, X_test, ventana, n_features)\n",
    "\n",
    "model, history = crea_gru(X.shape[2], ventana, X_train, X_test, y_train, y_test)\n",
    "grafica_loss_mae(history)\n",
    "\n",
    "pred_1step, real_1step = predice_1step(model, X_test, y_test, scaler, ventana, num_dias=ventana)\n",
    "pred_multistep = predice_multistep(model, X_test, scaler, ventana, num_dias_multi=ventana)\n",
    "\n",
    "metricas_1step = muestra_metricas(df_filtrado, ventana, pred_1step)\n",
    "metricas_multistep = muestra_metricas(df_filtrado, ventana, pred_multistep)\n",
    "\n",
    "grafico_mae = grafica_loss_mae(history)\n",
    "grafico_mae.write_image(f\"MODELS/GRU/Grafico LOSS_MAE_{ventana}.png\")\n",
    "grafica_predicciones(real_1step, pred_1step, pred_multistep)\n",
    "\n",
    "metricas = {\n",
    "\"modelo\" : f\"GRUmodel{ventana}\",\n",
    "\"prediccion\": [\"1-step\", \"multi-step\"],\n",
    "\"r2\": [metricas_1step[\"r2\"], metricas_multistep[\"r2\"]],\n",
    "\"mae_GWh\": [metricas_1step[\"mae_GWh\"], metricas_multistep[\"mae_GWh\"]],\n",
    "\"rmse_GWh\": [metricas_1step[\"rmse_GWh\"], metricas_multistep[\"rmse_GWh\"]]\n",
    "}\n",
    "\n",
    "df_metricas = pd.read_csv(\"MODELS/GRU/MetricasGRU.csv\")\n",
    "df_metricas_ventana = pd.DataFrame(metricas)\n",
    "df_metricas = pd.concat([df_metricas, df_metricas_ventana])\n",
    "\n",
    "df_metricas.to_csv(\"MODELS/GRU/MetricasGRU.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
